{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with logistic regression and support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already discussed, in suppervised learning we have a set of observations (points) with a label assigned to each of them. In particular, in a classification problem such labels are categorical and is our task to train an algorihtm to make future predictions and classify objects according to unobserved features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present a couple of exmaples to showcase the use of logisitic regression and SVM for classification problems. The intention is to show the capabilities that SVM offers when we have a problem that is not linearly separable. We also show how to use training and validation sets for tunning hyperparameters using grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start by loading the Python libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Typycal Python tools\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "import pylab\n",
    "from matplotlib import pyplot, cm\n",
    "\n",
    "# Import sci-kit learn libraries for ML: Linear Regression and SVM\n",
    "from sklearn import linear_model as linMod\n",
    "from sklearn import svm, grid_search\n",
    "# To split into training and test\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# To download dataset\n",
    "import wget\n",
    "\n",
    "# To show plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Example: Classify students into admitted/not admitted classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a group of students taking two exams as part of the process to being admitted into university. You should look at this example as a more elaborate admission process as opposed to the tradition mean of two exams: a committe evaluates the exams and makes a decision without you knowing what the criteria of their decision is. It is then your task to train a model to make future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Plotting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by plotting the dataset with the corresponding labels. For this purpose we first download the data and write the following function to read them into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget -O students_admitted.txt \\\n",
    "https://raw.githubusercontent.com/dataAllAround/data-for-classification/master/students_admitted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous command fails, uncomment the following lines and execute the cell containing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file_01 = \\\n",
    "#wget.download(\"https://raw.githubusercontent.com/dataAllAround/data-for-classification/master/students_admitted.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlotDataTXT():\n",
    "    \"\"\"\n",
    "    This function reads data from a txt file and plots them according to the labels (1 or 0) using different\n",
    "    shapes and colours.\n",
    "    \"\"\"\n",
    "    # Read data to data frame\n",
    "    df = pd.read_csv(\"students_admitted.txt\")\n",
    "    # Turn dataframe into matrix\n",
    "    data = df.values    \n",
    "    # We separate the given training set into positive (accepted) and negative (not accepted)\n",
    "    # according to \"1\" and \"0\" given in the training set.\n",
    "    positives  = data[data[:,2] == 1]\n",
    "    negatives  = data[data[:,2] == 0]\n",
    "    # We add the X and Y labels...\n",
    "    pyplot.xlabel(\"Exam 1 score\")\n",
    "    pyplot.ylabel(\"Exam 2 score\")\n",
    "    # ... and set the X and Y limit values of the plot.\n",
    "    pyplot.xlim([25, 105])\n",
    "    pyplot.ylim([25, 105])\n",
    "    # We generate the scatter plot according to the separation we just did.\n",
    "    # Plot negtive examples in yellow circles ...\n",
    "    pyplot.scatter( negatives[:, 0], negatives[:, 1], c='y', marker='o',\n",
    "                   s=50, linewidths=1, label=\"Not admitted\" )\n",
    "    # ... and positve examples in red crosses \n",
    "    pyplot.scatter( positives[:, 0], positives[:, 1], c='r', marker='+',\n",
    "                   s=50, linewidths=2, label=\"Admitted\" )\n",
    "    # And add legends\n",
    "    pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now simply execute the function to display plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PlotDataTXT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot suggests that we can separate the two classes with a linear boundary. Let's try to do so by means of logisit regression. We first define plotting functions to avoid extensive writing to have plotting capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Auxiliary plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions generate, respectively, plots of the training set and the boundary generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(data):\n",
    "    \"\"\"\n",
    "    This function plots training set with selected labels\n",
    "    \"\"\"\n",
    "    # We split data into positive and negative examples according to \n",
    "    # labels included in the dataset\n",
    "    positives = data[data[:, 2] == 1]\n",
    "    negatives = data[data[:, 2] == 0]\n",
    "    \n",
    "    # We add the X and Y labels to the plot\n",
    "    pyplot.xlabel(\"Exam 1 score\")\n",
    "    pyplot.ylabel(\"Exam 2 score\")\n",
    "    \n",
    "    # We set limits for X and Y axes \n",
    "    pyplot.xlim([25, 105])\n",
    "    pyplot.ylim([25, 105])\n",
    "    \n",
    "    # Plot negative examples in yellow circles ...\n",
    "    pyplot.scatter( negatives[:, 0], negatives[:, 1], c='y', marker='o', s=50,\n",
    "                   linewidths=1 )\n",
    "    # ... and positive examples in red crosses\n",
    "    pyplot.scatter( positives[:, 0], positives[:, 1], c='r', marker='+', s=50,\n",
    "                   linewidths=2)\n",
    "    \n",
    "\n",
    "def PlotBoundary( X, TrainedModel,color ):\n",
    "    \"\"\"\n",
    "    This function plots the boundary decision generated by the trained model\n",
    "    \"\"\"\n",
    "    # Generate evenly separated points for the X and Y axes....\n",
    "    x1plot = np.linspace( min(X[:, 0]), max(X[:, 0]), 100 )\n",
    "    x2plot = np.linspace( min(X[:, 1]), max(X[:, 1]), 100 )\n",
    "    # ... and generate grid based on them\n",
    "    X1, X2 = np.meshgrid( x1plot, x2plot )\n",
    "    \n",
    "    # Set an array of zeroes with the same shape\n",
    "    vals = np.zeros(X1.shape) \n",
    "   \n",
    "    # \n",
    "    aux_shape = X1.shape\n",
    "        \n",
    "    for i in range(aux_shape[1]):\n",
    "        # Concatenate values gotten for the grid\n",
    "        aux_X = np.c_[ X1[:, i], X2[:, i] ]\n",
    "        # Generate predicitions based on trained model\n",
    "        vals[:, i] = TrainedModel.predict( aux_X )\n",
    "    # Plot!\n",
    "    pyplot.contour( X1, X2, vals, colors=color )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Logistic regression and its associated boundary decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally generate the logistic regression model and plot its corresponding boundary decision. The following function\n",
    "reads the data, trains the model, makes predictions and plots the boundary. The only argument of this function is the regularization parameter. Remember that the regulariazation term helps to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LinRegr_Boundary(Cval,color='blue'):\n",
    "    \"\"\"\n",
    "    This function reads in the reciprocal of the regularization parameter to run\n",
    "    a logistic regression and plot the boundary decission.\n",
    "    Note.- Smaller Cval values specify stronger regularization.\n",
    "    \"\"\"\n",
    "    # Read data to dataframe\n",
    "    df = pd.read_csv(\"data/students_admitted.txt\")\n",
    "    # Get values from it to a numpy array\n",
    "    mat = df.values\n",
    "    # Split data into features and labels\n",
    "    X = mat[:,:2] ; y = mat[:,2]\n",
    "    \n",
    "    # Instantiate logistic regression\n",
    "    logreg = linMod.LogisticRegression(C=Cval)\n",
    "    # Fit model \n",
    "    logreg.fit(X, y )\n",
    "    # Plot training set...\n",
    "    plot( mat )\n",
    "    # ... and resulting separating line\n",
    "    PlotBoundary( X, logreg ,color)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we generate different classifiers according to different values of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LinRegr_Boundary(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LinRegr_Boundary(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LinRegr_Boundary(1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LinRegr_Boundary(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LinRegr_Boundary(1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Support vector machine (SVM) and its associated boundary decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do classification once again but this time with support vector machine (SVM) with linear kernel. The following function does something similar as the one defined before. Notice that this function requires, in addition to the regularization term, the type of kerned used for the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SVM_Boundary_01(Cval,sigmaVal,kernel_type,color='blue'):\n",
    "    \"\"\"\n",
    "    This function reads in the reciprocal of the regularization parameter to run\n",
    "    a logistic regression and plot the boundary decission.\n",
    "    Note.- Smaller Cval values specify stronger regularization.\n",
    "    \"\"\"\n",
    "    # Read data to dataframe\n",
    "    df = pd.read_csv(\"data/students_admitted.txt\")\n",
    "    # Get values from it to a numpy array\n",
    "    mat = df.values\n",
    "    # Split data into features and labels\n",
    "    X = mat[:,:2] ; y = mat[:,2]\n",
    "    \n",
    "    # gamma is actually inverse of sigma\n",
    "    svm_ = svm.SVC(C=Cval, kernel=kernel_type, gamma = 1.0 / sigmaVal ) \n",
    "    svm_.fit( X, y )\n",
    "\n",
    "\n",
    "    # Plot training set...\n",
    "    plot( mat )\n",
    "    # ... and resulting separating line\n",
    "    PlotBoundary( X, svm_ ,color)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by trying linear kernel for the classification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 10.\n",
    "sigmaVal = 2000.\n",
    "kernel_type = 'linear'\n",
    "SVM_Boundary_01(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, the perfomace of SVM is quite similar the that of logistic regression, as long as we use linear kernel. Let's try to improve the classification by using a non-linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the same parameters as before but this time we select a radial basis function to see if we improve the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 10.\n",
    "sigmaVal = 2000.\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_01(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly notice that the non-linearity of the kernel permits to have a better performance in the classifier. Let's plot both classifiers in the same graph to see the difference clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 10.\n",
    "sigmaVal = 2000.\n",
    "SVM_Boundary_01(Cval,sigmaVal,'linear','magenta')\n",
    "SVM_Boundary_01(Cval,sigmaVal,'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that by changing the type of kernel used and keeping all other parameters fixed we have a better classifier. This a simple way of seeing the capabilities of considering a kernel that is not linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## III. Example: Classify tumors into malignant/not malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explore a more challenging situation. The dataset explored in this example contains information regrading tumors detected in patients and we need to classify them into malignant and not malignant. As usual we first download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget -O tumors_classification.csv \\\n",
    "https://raw.githubusercontent.com/dataAllAround/data-for-classification/master/tumors_classification.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, if the previous cell fails, uncomment the lines below and execute the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_02 = \\\n",
    "wget.download(\"https://raw.githubusercontent.com/dataAllAround/data-for-classification/master/tumors_classification.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are written to plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotData( data, addLabel=True ):\n",
    "    # We separate the given training set into positive (malignant) and negative\n",
    "    # (not malignant)\n",
    "    # according to \"1\" and \"0\" given in the training set.\n",
    "    positives  = data[data[:,2] == 1]\n",
    "    negatives  = data[data[:,2] == 0]\n",
    "    # We add the X and Y labels...\n",
    "    pyplot.xlabel(\"Feature A\")\n",
    "    pyplot.ylabel(\"Feature B\")\n",
    "    # ... and set the X and Y limit values of the plot.\n",
    "    pyplot.xlim([0, 1.05])\n",
    "    pyplot.ylim([0.35, 1])\n",
    "    \n",
    "    # We make a distinction whether to include labels\n",
    "    if addLabel:\n",
    "        # We generate the scatter plot according to the separation we just did.\n",
    "        pyplot.scatter( negatives[:, 0], negatives[:, 1], c='y', marker='o', s=50,\n",
    "                   linewidths=1 , label=\"Not malignant\" )\n",
    "        pyplot.scatter( positives[:, 0], positives[:, 1], c='r', marker='+', s=50,\n",
    "                   linewidths=2 , label=\"Malignant\" )\n",
    "        # And add legends\n",
    "        pyplot.legend()\n",
    "    \n",
    "    else:\n",
    "         # We generate the scatter plot according to the separation we just did.\n",
    "        pyplot.scatter( negatives[:, 0], negatives[:, 1], c='y', marker='o', s=50,\n",
    "                   linewidths=1 )\n",
    "        pyplot.scatter( positives[:, 0], positives[:, 1], c='r', marker='+', s=50,\n",
    "                   linewidths=2 )\n",
    "        \n",
    "        \n",
    "def PlotDataFrame():\n",
    "    df = pd.read_csv(\"tumors_classification.csv\" , index_col=False)\n",
    "    mat = df.values[:,1:]\n",
    "    mat[:,2] = mat[:,2].astype(int)\n",
    "    plotData( mat , addLabel= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PlotDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the previous plot it is not possible to classify data with a linear boundary. Let's then apply SVM directly with radial basis function; we write the following function for such goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_Boundary_02(Cval,sigmaVal,kernel_type,color='blue'):\n",
    "    \"\"\"\n",
    "    This function reads in the reciprocal of the regularization parameter to run\n",
    "    a logistic regression and plot the boundary decission.\n",
    "    Note.- Smaller Cval values specify stronger regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(\"tumors_classification.csv\" , index_col=False)\n",
    "    mat = df.values[:,1:]\n",
    "    mat[:,2] = mat[:,2].astype(int)\n",
    "    #plotData( mat )\n",
    "    \n",
    "    \n",
    "    # Split data into features and labels\n",
    "    X = mat[:,:2] ; y = mat[:,2]\n",
    "    \n",
    "    # gamma is actually inverse of sigma\n",
    "    svm_ = svm.SVC(C=Cval, kernel=kernel_type, gamma = 1.0 / sigmaVal ) \n",
    "    svm_.fit( X, y )\n",
    "\n",
    "\n",
    "    # Plot training set...\n",
    "    plotData( mat , addLabel=False )\n",
    "    # ... and resulting separating line\n",
    "    PlotBoundary( X, svm_ ,color)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the effect of parameters Cval and sigmaVal in the classifier by trying different values for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cval = 1.\n",
    "sigmaVal = 1.\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_02(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 1.\n",
    "sigmaVal = 1e-1\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_02(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 1.\n",
    "sigmaVal = 1e-2\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_02(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 1.\n",
    "sigmaVal = 1e-3\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_02(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cval = 1.\n",
    "sigmaVal = 1e-4\n",
    "kernel_type = 'rbf'\n",
    "SVM_Boundary_02(Cval,sigmaVal,kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous trials we see that indeed some values of the parameters provide a very good classifier. A natural question to ask is if there is a process by which we can choose \"the best\" values of such parameters. Notice that, so far, we have used all the given data available to build the classifier. In what follows we will use subsets of the data given to train our model and choose the best set of parameters. The process of spliting the dataset is known as **cross-validation** and that of finding \"the best\" set of parameters is known as **grid search**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Cross-validation and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split our data in three subsets: train (60%), validation (20%) and test (20%). We will explain the purpose of each as we show the following cells. We first set a seed to make the split of data as deterministic as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 2016\n",
    "random_state = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split our dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first read data to a data frame\n",
    "df = pd.read_csv(\"tumors_classification.csv\" , index_col=False)\n",
    "# Grab the meaningful data and transform them accordingly.\n",
    "mat = df.values[:,1:]\n",
    "mat[:,2] = mat[:,2].astype(int)\n",
    "print \"Total number of records \" , mat.shape[0]\n",
    "# We split data into train (60 %) and an auxiliary dataset (40%)...\n",
    "train_, test_aux = train_test_split(mat, train_size=0.6,random_state=random_state)\n",
    "# ... and we split, in turn, this auxiliary dataset into validation (20%) and test sets (20%)\n",
    "val_, test_ = train_test_split(test_aux, train_size=0.5,random_state=random_state)\n",
    "print \"Records in training set \" , train_.shape[0]\n",
    "print \"Records in validation set \" , val_.shape[0]\n",
    "print \"Records in test set \" , test_.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot first the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotData( train_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, a linear classifier will not be able to make an accpetable classification. It is time to put SVM to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first prepare the three set of data as needed by scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into features and labels. We do this for training, validation and test set\n",
    "X_train = train_[:,:2] ; y_train = train_[:,2]\n",
    "X_val = val_[:,:2] ; y_val = val_[:,2]\n",
    "X_test = test_[:,:2] ; y_test = test_[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will use the training and validation set. The training set will be used with different parameters (values of Cval and sigmaVal) and for each of them we will get the score by using the validation set. The ultimate set of parameters chosen for future predicitons will be the one that gives the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty lists to store scores and parameters...\n",
    "list_score = []\n",
    "list_par = []\n",
    "# List of possible Cval values ...\n",
    "Cvals = [1e-2,1e-1,1.,1e1,1e2]\n",
    "# and possible sigmaVal values ...\n",
    "sigmaVals = [1e-2,1e-1,1.,1e1,1e2]\n",
    "# We do grid search in Cval ...\n",
    "for Cval in Cvals:\n",
    "    # ... and sigmaVal\n",
    "    for sigmaVal in sigmaVals:\n",
    "        # For the pair in turn we instantiate the model...\n",
    "        svm_ = svm.SVC(C=Cval, kernel='rbf', gamma = 1.0 / sigmaVal ) \n",
    "        # ... train it with the training set ...\n",
    "        svm_.fit( X_train, y_train )\n",
    "        # ... and evaluate perfomance in the validation set ...\n",
    "        score_ = svm_.score( X_val,y_val )\n",
    "        # If desired, print values...\n",
    "        #print Cval, sigmaVal, score_\n",
    "        # Add score and parameters to the corresponding lists...\n",
    "        list_score.append(score_)\n",
    "        list_par.append([Cval,sigmaVal])\n",
    "\n",
    "# Store maximum score -- SVM is about maximizing margin \n",
    "max_score = max(list_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Maximum score \" ,  max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find indices that correspond to the maximum score in list_score and see what parameters they correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_parameters = []\n",
    "indices_ = [i for i, j in enumerate(list_score) if float(j) == max_score]\n",
    "for k in indices_:\n",
    "    print list_par[k]\n",
    "    selected_parameters.append(list_par[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **MAY** observe more than one value of Cval provide the same score. Let's visualize results in the test set. For this purpouse we first write the function that does the plotting needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_Boundary_Data(Cval,sigmaVal,kernel_type,X,y,color='blue'):\n",
    "    \"\"\"\n",
    "    This function reads in the reciprocal of the regularization parameter to run\n",
    "    a logistic regression and plot the boundary decission.\n",
    "    Note.- Smaller Cval values specify stronger regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # gamma is actually inverse of sigma\n",
    "    svm_ = svm.SVC(C=Cval, kernel=kernel_type, gamma = 1.0 / sigmaVal ) \n",
    "    svm_.fit( X, y )\n",
    "\n",
    "    mat = np.c_[X,y]\n",
    "    # Plot training set...\n",
    "    plotData( mat , addLabel=False )\n",
    "    # ... and resulting separating line\n",
    "    PlotBoundary( X, svm_,color )\n",
    "    # Perform classification on X\n",
    "    #print svm_.predict(X)\n",
    "    #print y - svm_.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot, in different colours, boundary for every pair of parameters found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors_list = ['blue','magenta','green','orange','black']\n",
    "kernel_type = 'rbf'\n",
    "for k,par in enumerate(selected_parameters):\n",
    "    print \"Parameters Cval and sigmaVal\" ,par, \" in color \" , colors_list[k]\n",
    "    Cval,sigmaVal = par\n",
    "    SVM_Boundary_Data(Cval,sigmaVal,kernel_type,X_test,y_test,color=colors_list[k])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind this process is strictly visual and should not been consider as the final process to find the best classifier. Moreover, in this process we focused on finding the classifier that separates the best both classes (maximum margine). But what does it actually mean to have the best classifier? \n",
    "In what follows we will do a more formal, elegant and exhaustive search on the parameter set. This will be done with a scikit-learn built-in functionality to do grid search and it will be focused on statistical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Exhaustive search over specified parameter values for an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will do a more exahustive search on the parameter set. For this goal we set the following list of dictionaries. Each dictionary in the list will contain type of kernel, value of C and value of gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1/1e-2,1/1e-1,1.,1/1e1,1/1e2],\n",
    "                     'C': [1e-2,1e-1,1.,1e1,1e2]},\n",
    "                    {'kernel': ['linear'], 'C': [1e-2,1e-1,1.,1e1,1e2]}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are interested in learning more about grid search with cross-validation feel free to visit the following website"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GridSearchCV for more info:\n",
    "www.scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize grid_search object \n",
    "print(\"Object Information:\")\n",
    "clf = grid_search.GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=4,scoring='accuracy')\n",
    "#train object \n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing of the following cell will do the exahustive search we have mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every set of elements in grid scores of the classifier...\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    # ... print parameters ...\n",
    "    print('Parameter Values',params)\n",
    "    # ... and mean accuracy and twice the standard deviation\n",
    "    print(\"Mean accuracy\",mean_score, \"2X Standard Deviation\" , 2*scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally let scikit learn letting us know what the best  classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the hands-on session. We hope you enjoyed it and learn a bit more main differences between logistic regression and support vector machine. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
